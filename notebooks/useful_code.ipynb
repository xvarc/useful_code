{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Admin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making a directory tree with cookie cutter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# UPDATE: I have my own cookie cutter directory in my github now so I can link to that one. \n",
    "\n",
    "\n",
    "#run from terminal \n",
    "\n",
    "# $ pip install cookiecutter\n",
    "# $ cd path_to_my_project_folder\n",
    "# # pick your own flavour of cookie\n",
    "# $ cookiecutter https://github.com/drivendata/cookiecutter-data-science\n",
    "\n",
    "# my one\n",
    "# cookiecutter https://github.com/xvarc/my_cookiecutter \n",
    "\n",
    "\n",
    "# the link at the bottom is a a recommended github folder structure. custom folder structures can also be made if necessary. \n",
    "\n",
    "#This is the folder structure:\n",
    "\n",
    "# ├── README.md          <- Front page of the project. Let everyone \n",
    "# │                         know the major points.\n",
    "# │\n",
    "# ├── models             <- Trained and serialized models, model\n",
    "# │                         predictions, or model summaries.\n",
    "# │\n",
    "# ├── notebooks          <- Jupyter notebooks. Use set naming\n",
    "# │                         E.g. `1.2-rd-data-exploration`.\n",
    "# │\n",
    "# ├── reports            <- HTML, PDF, and LaTeX.\n",
    "# │   └── figures        <- Generated figures.\n",
    "# │\n",
    "# ├── requirements.txt   <- File for reproducing the environment\n",
    "# │                         `$ pip freeze > requirements.txt`\n",
    "# ├── data\n",
    "# │   ├── external       <- Third party sources.\n",
    "# │   ├── interim        <- In-progress intermediate data.\n",
    "# │   ├── processed      <- The final data sets for modelling.\n",
    "# │   └── raw            <- The original, immutable data.\n",
    "# │\n",
    "# └── src                <- Source code for use in this project.\n",
    "#     ├── __init__.py    <- Makes src a Python module. \n",
    "#     │\n",
    "#     ├── custom_func.py <- Various custom functions to import.\n",
    "#     │\n",
    "#     ├── data           <- Scripts to download or generate data.\n",
    "#     │   └── make_dataset.py\n",
    "#     │\n",
    "#     ├── features       <- Scripts raw data into features for\n",
    "#     │   │                 modeling.\n",
    "#     │   └── build_features.py\n",
    "#     │\n",
    "#     ├── models         <- Scripts to train models and then use\n",
    "#     │   │                 trained models to make predictions.\n",
    "#     │   │                 \n",
    "#     │   ├── predict_model.py\n",
    "#     │   └── train_model.py\n",
    "#     │\n",
    "#     └── viz            <- Scripts to create visualizations.            \n",
    "#         └── viz.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Python Basics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accessing the command line "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! echo my ass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introspectives. Functions in python that let me understand objects better\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type()\n",
    "dir()\n",
    "id()\n",
    "getattr()\n",
    "hasattr()\n",
    "globals()\n",
    "locals()\n",
    "callable()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns \n",
    "import matplotlib as mpl \n",
    "import matplotlib.pyplot as plt\n",
    "from os import path\n",
    "from PIL import Image\n",
    "from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "from matplotlib.ticker import StrMethodFormatter\n",
    "plt.rcParams.update({'font.size': 20})\n",
    "mpl.rc('font', **{'sans-serif' : 'Arial',\n",
    "                         'family' : 'sans-serif'})\n",
    "\n",
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.width', 150)\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "pip install japanize-matplotlib\n",
    "import japanize_matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#scikit imports\n",
    "\n",
    "# # sklearn preproc\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "# from sklearn.pipeline import Pipeline, make_pipeline\n",
    "# from sklearn.compose import ColumnTransformer, make_column_transformer, make_column_selector\n",
    "# from sklearn.impute import SimpleImputer, KNNImputer\n",
    "# from sklearn.preprocessing import RobustScaler, OneHotEncoder, OrdinalEncoder\n",
    "# from sklearn.metrics import make_scorer\n",
    "\n",
    "# from sklearn.model_selection import cross_val_score\n",
    "# from sklearn.neighbors import KNeighborsRegressor\n",
    "# from sklearn.linear_model import Ridge, Lasso, LinearRegression\n",
    "# from sklearn.model_selection import RandomizedSearchCV\n",
    "# from sklearn.svm import SVR\n",
    "# from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
    "# from sklearn.ensemble import AdaBoostRegressor\n",
    "# from sklearn.tree import DecisionTreeRegressor\n",
    "# from sklearn.ensemble import VotingRegressor\n",
    "# from sklearn.ensemble import GradientBoostingRegressor\n",
    "# from sklearn.ensemble import StackingRegressor\n",
    "# from sklearn.ensemble import RandomForestRegressor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Handling Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Downloading Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Downloading open data sets\n",
    "\n",
    "OpenDataSets allows us to download the dataset from a notebook. It will create a folder with the dataset in the same folder where your notebook saves some time. Cool, right?\n",
    "To use it, you just need to type pip install opendataset in your terminal. Then, you need to import it to the notebook by typing import opendatasets as od, and you are good to go. Kaggle will ask for your credentials, but you can easily get it in your Kaggle profile page. In the example below, I want to download the famous heart attack dataset. Here is the code you will need:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import opendatasets as od\n",
    "\n",
    "\n",
    "#od.download(\"https://www.kaggle.com/rashikrahmanpritom/heart-attack-analysis-prediction-dataset\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Downloading with kaggles CLI (command line interface)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# install kaggle\n",
    "!pip install kaggle\n",
    "\n",
    "# download using name of person who uploaded/name of data file. you can find it by clicking on the \"copy api link\" in the top right of the data page\n",
    "!kaggle datasets download -d mathan/fifa-2018-match-statistics \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load in the dataframe\n",
    "df = pd.read_csv(\"where_heard.csv\", index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#saving data\n",
    "df[\"where_heard\"].to_csv('where_heard.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Handling Data Frames/Columns etc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Printing Big Data Frames\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_full(x):\n",
    "    pd.set_option('display.max_rows', len(x))\n",
    "    pd.set_option('display.max_columns', None)\n",
    "    pd.set_option('display.width', 2000)\n",
    "    pd.set_option('display.float_format', '{:20,.2f}'.format)\n",
    "    pd.set_option('display.max_colwidth', None)\n",
    "    print(x)\n",
    "    pd.reset_option('display.max_rows')\n",
    "    pd.reset_option('display.max_columns')\n",
    "    pd.reset_option('display.width')\n",
    "    pd.reset_option('display.float_format')\n",
    "    pd.reset_option('display.max_colwidth')\n",
    "    \n",
    "print_full(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Concatanating columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#concatanating two columns or lists into a dataframe. concat takes series and dataframe objects so you should use pd.series to convert. \n",
    "\n",
    "ID = pd.Series([1,2,3])\n",
    "price = pd.Series([1,2,3])\n",
    "\n",
    "pd.concat([ID,price],axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Selecting and filtering data\n",
    "\n",
    "\n",
    "you can print columns based on their data type, speeding up the process of isolating categoric or numerical data. single or multiple data types can be selected. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Drop columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = [1,2,4,5,12]\n",
    "# you can also copy the names of the columns in there ,from df.columns. \n",
    "df.drop(df.columns[cols],axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dropping Data Types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-89aaa105ef01>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect_dtypes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minclude\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'int64'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "df.select_dtypes(include='int64')\n",
    "\n",
    "# this selects only numerical data with select datatypes\n",
    "# the second drop methods drops columns that i don't want to have in my data\n",
    "\n",
    "df_numbers_only = df.select_dtypes(include = ['float64','int64']).drop(columns=['donation_settlement_interval_code','donation_amount'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Replacing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'payment_amount'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m~/.pyenv/versions/3.8.6/envs/lewagon/lib/python3.8/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   3079\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3080\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3081\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'payment_amount'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-37-62704e9de3ca>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'payment_amount'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m''\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnan\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minplace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.pyenv/versions/3.8.6/envs/lewagon/lib/python3.8/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3022\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnlevels\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3023\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_multilevel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3024\u001b[0;31m             \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3025\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mis_integer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3026\u001b[0m                 \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.8.6/envs/lewagon/lib/python3.8/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   3080\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3081\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3082\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3083\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3084\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtolerance\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'payment_amount'"
     ]
    }
   ],
   "source": [
    "df['payment_amount'].replace('', np.nan, inplace=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Formatting Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Last Week Dates: {min(df.submission_date.dt.date)} - { max(df.submission_date.dt.date)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cleaning "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Automating Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "QuickDA is an easy-to-use, intuitive low-code library that allows you to perform data cleaning, data exploration, and data visualization with a few lines of code. In fact, we will be using only one line of code most of the time. I have created a little project to demonstrate how powerful QuickDa can be. You can find the notebook here.\n",
    "\n",
    "https://towardsdatascience.com/save-hours-of-work-doing-a-complete-eda-with-a-few-lines-of-code-45de2e60f257\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: quickda in /Users/xv/.pyenv/versions/3.8.6/envs/lewagon/lib/python3.8/site-packages (0.2.2)\n",
      "Requirement already satisfied: ppscore in /Users/xv/.pyenv/versions/3.8.6/envs/lewagon/lib/python3.8/site-packages (from quickda) (1.2.0)\n",
      "Requirement already satisfied: seaborn in /Users/xv/.pyenv/versions/3.8.6/envs/lewagon/lib/python3.8/site-packages (from quickda) (0.11.1)\n",
      "Requirement already satisfied: pandas-profiling in /Users/xv/.pyenv/versions/3.8.6/envs/lewagon/lib/python3.8/site-packages (from quickda) (3.0.0)\n",
      "Requirement already satisfied: pandas in /Users/xv/.pyenv/versions/3.8.6/envs/lewagon/lib/python3.8/site-packages (from quickda) (1.2.4)\n",
      "Requirement already satisfied: plotly in /Users/xv/.pyenv/versions/3.8.6/envs/lewagon/lib/python3.8/site-packages (from quickda) (4.14.3)\n",
      "Requirement already satisfied: matplotlib in /Users/xv/.pyenv/versions/3.8.6/envs/lewagon/lib/python3.8/site-packages (from quickda) (3.4.2)\n",
      "Requirement already satisfied: numpy in /Users/xv/.pyenv/versions/3.8.6/envs/lewagon/lib/python3.8/site-packages (from quickda) (1.18.5)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /Users/xv/.pyenv/versions/3.8.6/envs/lewagon/lib/python3.8/site-packages (from matplotlib->quickda) (1.3.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /Users/xv/.pyenv/versions/3.8.6/envs/lewagon/lib/python3.8/site-packages (from matplotlib->quickda) (2.8.1)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in /Users/xv/.pyenv/versions/3.8.6/envs/lewagon/lib/python3.8/site-packages (from matplotlib->quickda) (2.4.7)\n",
      "Requirement already satisfied: cycler>=0.10 in /Users/xv/.pyenv/versions/3.8.6/envs/lewagon/lib/python3.8/site-packages (from matplotlib->quickda) (0.10.0)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /Users/xv/.pyenv/versions/3.8.6/envs/lewagon/lib/python3.8/site-packages (from matplotlib->quickda) (8.2.0)\n",
      "Requirement already satisfied: six in /Users/xv/.pyenv/versions/3.8.6/envs/lewagon/lib/python3.8/site-packages (from cycler>=0.10->matplotlib->quickda) (1.14.0)\n",
      "Requirement already satisfied: pytz>=2017.3 in /Users/xv/.pyenv/versions/3.8.6/envs/lewagon/lib/python3.8/site-packages (from pandas->quickda) (2021.1)\n",
      "Requirement already satisfied: PyYAML>=5.0.0 in /Users/xv/.pyenv/versions/3.8.6/envs/lewagon/lib/python3.8/site-packages (from pandas-profiling->quickda) (5.4.1)\n",
      "Requirement already satisfied: jinja2>=2.11.1 in /Users/xv/.pyenv/versions/3.8.6/envs/lewagon/lib/python3.8/site-packages (from pandas-profiling->quickda) (3.0.0)\n",
      "Requirement already satisfied: missingno>=0.4.2 in /Users/xv/.pyenv/versions/3.8.6/envs/lewagon/lib/python3.8/site-packages (from pandas-profiling->quickda) (0.4.2)\n",
      "Requirement already satisfied: phik>=0.11.1 in /Users/xv/.pyenv/versions/3.8.6/envs/lewagon/lib/python3.8/site-packages (from pandas-profiling->quickda) (0.11.2)\n",
      "Requirement already satisfied: requests>=2.24.0 in /Users/xv/.pyenv/versions/3.8.6/envs/lewagon/lib/python3.8/site-packages (from pandas-profiling->quickda) (2.25.1)\n",
      "Requirement already satisfied: joblib in /Users/xv/.pyenv/versions/3.8.6/envs/lewagon/lib/python3.8/site-packages (from pandas-profiling->quickda) (1.0.1)\n",
      "Requirement already satisfied: htmlmin>=0.1.12 in /Users/xv/.pyenv/versions/3.8.6/envs/lewagon/lib/python3.8/site-packages (from pandas-profiling->quickda) (0.1.12)\n",
      "Requirement already satisfied: visions[type_image_path]==0.7.1 in /Users/xv/.pyenv/versions/3.8.6/envs/lewagon/lib/python3.8/site-packages (from pandas-profiling->quickda) (0.7.1)\n",
      "Requirement already satisfied: scipy>=1.4.1 in /Users/xv/.pyenv/versions/3.8.6/envs/lewagon/lib/python3.8/site-packages (from pandas-profiling->quickda) (1.6.3)\n",
      "Requirement already satisfied: pydantic>=1.8.1 in /Users/xv/.pyenv/versions/3.8.6/envs/lewagon/lib/python3.8/site-packages (from pandas-profiling->quickda) (1.8.2)\n",
      "Requirement already satisfied: tqdm>=4.48.2 in /Users/xv/.pyenv/versions/3.8.6/envs/lewagon/lib/python3.8/site-packages (from pandas-profiling->quickda) (4.60.0)\n",
      "Requirement already satisfied: tangled-up-in-unicode==0.1.0 in /Users/xv/.pyenv/versions/3.8.6/envs/lewagon/lib/python3.8/site-packages (from pandas-profiling->quickda) (0.1.0)\n",
      "Requirement already satisfied: attrs>=19.3.0 in /Users/xv/.pyenv/versions/3.8.6/envs/lewagon/lib/python3.8/site-packages (from visions[type_image_path]==0.7.1->pandas-profiling->quickda) (21.2.0)\n",
      "Requirement already satisfied: bottleneck in /Users/xv/.pyenv/versions/3.8.6/envs/lewagon/lib/python3.8/site-packages (from visions[type_image_path]==0.7.1->pandas-profiling->quickda) (1.3.2)\n",
      "Requirement already satisfied: multimethod==1.4 in /Users/xv/.pyenv/versions/3.8.6/envs/lewagon/lib/python3.8/site-packages (from visions[type_image_path]==0.7.1->pandas-profiling->quickda) (1.4)\n",
      "Requirement already satisfied: networkx>=2.4 in /Users/xv/.pyenv/versions/3.8.6/envs/lewagon/lib/python3.8/site-packages (from visions[type_image_path]==0.7.1->pandas-profiling->quickda) (2.5.1)\n",
      "Requirement already satisfied: imagehash in /Users/xv/.pyenv/versions/3.8.6/envs/lewagon/lib/python3.8/site-packages (from visions[type_image_path]==0.7.1->pandas-profiling->quickda) (4.2.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0.0rc2 in /Users/xv/.pyenv/versions/3.8.6/envs/lewagon/lib/python3.8/site-packages (from jinja2>=2.11.1->pandas-profiling->quickda) (2.0.0)\n",
      "Requirement already satisfied: decorator<5,>=4.3 in /Users/xv/.pyenv/versions/3.8.6/envs/lewagon/lib/python3.8/site-packages (from networkx>=2.4->visions[type_image_path]==0.7.1->pandas-profiling->quickda) (4.4.2)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Users/xv/.pyenv/versions/3.8.6/envs/lewagon/lib/python3.8/site-packages (from pydantic>=1.8.1->pandas-profiling->quickda) (3.7.4.3)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in /Users/xv/.pyenv/versions/3.8.6/envs/lewagon/lib/python3.8/site-packages (from requests>=2.24.0->pandas-profiling->quickda) (4.0.0)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/xv/.pyenv/versions/3.8.6/envs/lewagon/lib/python3.8/site-packages (from requests>=2.24.0->pandas-profiling->quickda) (1.26.4)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /Users/xv/.pyenv/versions/3.8.6/envs/lewagon/lib/python3.8/site-packages (from requests>=2.24.0->pandas-profiling->quickda) (2.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/xv/.pyenv/versions/3.8.6/envs/lewagon/lib/python3.8/site-packages (from requests>=2.24.0->pandas-profiling->quickda) (2021.5.30)\n",
      "Requirement already satisfied: PyWavelets in /Users/xv/.pyenv/versions/3.8.6/envs/lewagon/lib/python3.8/site-packages (from imagehash->visions[type_image_path]==0.7.1->pandas-profiling->quickda) (1.1.1)\n",
      "Requirement already satisfied: retrying>=1.3.3 in /Users/xv/.pyenv/versions/3.8.6/envs/lewagon/lib/python3.8/site-packages (from plotly->quickda) (1.3.3)\n",
      "Requirement already satisfied: scikit-learn<1.0.0,>=0.20.2 in /Users/xv/.pyenv/versions/3.8.6/envs/lewagon/lib/python3.8/site-packages (from ppscore->quickda) (0.24.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /Users/xv/.pyenv/versions/3.8.6/envs/lewagon/lib/python3.8/site-packages (from scikit-learn<1.0.0,>=0.20.2->ppscore->quickda) (2.1.0)\n"
     ]
    }
   ],
   "source": [
    "#quickda\n",
    "!pip3 install quickda\n",
    "\n",
    "\n",
    "# Importing libraries\n",
    "import pandas as pd\n",
    "from quickda.explore_data import *\n",
    "from quickda.clean_data import *\n",
    "from quickda.explore_numeric import *\n",
    "from quickda.explore_categoric import *\n",
    "from quickda.explore_numeric_categoric import *\n",
    "from quickda.explore_time_series import *\n",
    "\n",
    "df = pd.read_csv('datasets/ml/FIFA 2018 Statistics.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dtypes</th>\n",
       "      <th>count</th>\n",
       "      <th>null_sum</th>\n",
       "      <th>null_pct</th>\n",
       "      <th>nunique</th>\n",
       "      <th>min</th>\n",
       "      <th>25%</th>\n",
       "      <th>50%</th>\n",
       "      <th>75%</th>\n",
       "      <th>max</th>\n",
       "      <th>mean</th>\n",
       "      <th>median</th>\n",
       "      <th>std</th>\n",
       "      <th>skew</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1st Goal</th>\n",
       "      <td>float64</td>\n",
       "      <td>94</td>\n",
       "      <td>34</td>\n",
       "      <td>0.266</td>\n",
       "      <td>56</td>\n",
       "      <td>1.0</td>\n",
       "      <td>18.25</td>\n",
       "      <td>39.0</td>\n",
       "      <td>54.75</td>\n",
       "      <td>90.0</td>\n",
       "      <td>39.457447</td>\n",
       "      <td>39.0</td>\n",
       "      <td>24.496506</td>\n",
       "      <td>0.346921</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Attempts</th>\n",
       "      <td>int64</td>\n",
       "      <td>128</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>24</td>\n",
       "      <td>3</td>\n",
       "      <td>9.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>26</td>\n",
       "      <td>12.59375</td>\n",
       "      <td>12.0</td>\n",
       "      <td>5.245827</td>\n",
       "      <td>0.653512</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Ball Possession %</th>\n",
       "      <td>int64</td>\n",
       "      <td>128</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>43</td>\n",
       "      <td>25</td>\n",
       "      <td>42.0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>58.0</td>\n",
       "      <td>75</td>\n",
       "      <td>49.992188</td>\n",
       "      <td>50.0</td>\n",
       "      <td>10.444074</td>\n",
       "      <td>0.002231</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Blocked</th>\n",
       "      <td>int64</td>\n",
       "      <td>128</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>1.75</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>10</td>\n",
       "      <td>3.359375</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.403195</td>\n",
       "      <td>0.953987</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Corners</th>\n",
       "      <td>int64</td>\n",
       "      <td>128</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>11</td>\n",
       "      <td>4.71875</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2.446072</td>\n",
       "      <td>0.306062</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <td>object</td>\n",
       "      <td>128</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>25</td>\n",
       "      <td>01-07-2018</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>30-06-2018</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Distance Covered (Kms)</th>\n",
       "      <td>int64</td>\n",
       "      <td>128</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>35</td>\n",
       "      <td>80</td>\n",
       "      <td>101.0</td>\n",
       "      <td>104.5</td>\n",
       "      <td>109.0</td>\n",
       "      <td>148</td>\n",
       "      <td>106.664062</td>\n",
       "      <td>104.5</td>\n",
       "      <td>11.749537</td>\n",
       "      <td>1.773204</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Fouls Committed</th>\n",
       "      <td>int64</td>\n",
       "      <td>128</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>21</td>\n",
       "      <td>5</td>\n",
       "      <td>10.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>25</td>\n",
       "      <td>13.546875</td>\n",
       "      <td>13.0</td>\n",
       "      <td>4.619131</td>\n",
       "      <td>0.268439</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Free Kicks</th>\n",
       "      <td>int64</td>\n",
       "      <td>128</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>21</td>\n",
       "      <td>5</td>\n",
       "      <td>11.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>26</td>\n",
       "      <td>14.890625</td>\n",
       "      <td>15.0</td>\n",
       "      <td>4.724262</td>\n",
       "      <td>0.100006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Goal Scored</th>\n",
       "      <td>int64</td>\n",
       "      <td>128</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>6</td>\n",
       "      <td>1.320312</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.156519</td>\n",
       "      <td>1.145702</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Goals in PSO</th>\n",
       "      <td>int64</td>\n",
       "      <td>128</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4</td>\n",
       "      <td>0.203125</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.807049</td>\n",
       "      <td>3.903529</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Man of the Match</th>\n",
       "      <td>object</td>\n",
       "      <td>128</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>2</td>\n",
       "      <td>No</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>Yes</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Off-Target</th>\n",
       "      <td>int64</td>\n",
       "      <td>128</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>11</td>\n",
       "      <td>1</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>11</td>\n",
       "      <td>5.273438</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2.409675</td>\n",
       "      <td>0.28325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Offsides</th>\n",
       "      <td>int64</td>\n",
       "      <td>128</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>5</td>\n",
       "      <td>1.34375</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.193404</td>\n",
       "      <td>0.829584</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>On-Target</th>\n",
       "      <td>int64</td>\n",
       "      <td>128</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.5</td>\n",
       "      <td>5.0</td>\n",
       "      <td>12</td>\n",
       "      <td>3.914062</td>\n",
       "      <td>3.5</td>\n",
       "      <td>2.234403</td>\n",
       "      <td>0.802058</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Opponent</th>\n",
       "      <td>object</td>\n",
       "      <td>128</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>32</td>\n",
       "      <td>Argentina</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>Uruguay</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Own goal Time</th>\n",
       "      <td>float64</td>\n",
       "      <td>12</td>\n",
       "      <td>116</td>\n",
       "      <td>0.906</td>\n",
       "      <td>11</td>\n",
       "      <td>12.0</td>\n",
       "      <td>21.75</td>\n",
       "      <td>35.0</td>\n",
       "      <td>75.75</td>\n",
       "      <td>90.0</td>\n",
       "      <td>45.833333</td>\n",
       "      <td>35.0</td>\n",
       "      <td>29.978275</td>\n",
       "      <td>0.50659</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Own goals</th>\n",
       "      <td>float64</td>\n",
       "      <td>12</td>\n",
       "      <td>116</td>\n",
       "      <td>0.906</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PSO</th>\n",
       "      <td>object</td>\n",
       "      <td>128</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>2</td>\n",
       "      <td>No</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>Yes</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Pass Accuracy %</th>\n",
       "      <td>int64</td>\n",
       "      <td>128</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>25</td>\n",
       "      <td>67</td>\n",
       "      <td>79.0</td>\n",
       "      <td>83.0</td>\n",
       "      <td>87.0</td>\n",
       "      <td>94</td>\n",
       "      <td>82.554688</td>\n",
       "      <td>83.0</td>\n",
       "      <td>5.933766</td>\n",
       "      <td>-0.699259</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Passes</th>\n",
       "      <td>int64</td>\n",
       "      <td>128</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>110</td>\n",
       "      <td>189</td>\n",
       "      <td>351.0</td>\n",
       "      <td>462.0</td>\n",
       "      <td>555.25</td>\n",
       "      <td>1137</td>\n",
       "      <td>462.648438</td>\n",
       "      <td>462.0</td>\n",
       "      <td>151.186311</td>\n",
       "      <td>0.773985</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Red</th>\n",
       "      <td>int64</td>\n",
       "      <td>128</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.015625</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.124507</td>\n",
       "      <td>7.904196</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Round</th>\n",
       "      <td>object</td>\n",
       "      <td>128</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>6</td>\n",
       "      <td>3rd Place</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>Semi- Finals</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Saves</th>\n",
       "      <td>int64</td>\n",
       "      <td>128</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>9</td>\n",
       "      <td>2.726562</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.049447</td>\n",
       "      <td>0.945214</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Team</th>\n",
       "      <td>object</td>\n",
       "      <td>128</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>32</td>\n",
       "      <td>Argentina</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>Uruguay</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Yellow &amp; Red</th>\n",
       "      <td>int64</td>\n",
       "      <td>128</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.015625</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.124507</td>\n",
       "      <td>7.904196</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Yellow Card</th>\n",
       "      <td>int64</td>\n",
       "      <td>128</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>6</td>\n",
       "      <td>1.695312</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.325454</td>\n",
       "      <td>0.805754</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         dtypes  count  null_sum  null_pct  nunique  \\\n",
       "1st Goal                float64     94        34     0.266       56   \n",
       "Attempts                  int64    128         0     0.000       24   \n",
       "Ball Possession %         int64    128         0     0.000       43   \n",
       "Blocked                   int64    128         0     0.000       11   \n",
       "Corners                   int64    128         0     0.000       12   \n",
       "Date                     object    128         0     0.000       25   \n",
       "Distance Covered (Kms)    int64    128         0     0.000       35   \n",
       "Fouls Committed           int64    128         0     0.000       21   \n",
       "Free Kicks                int64    128         0     0.000       21   \n",
       "Goal Scored               int64    128         0     0.000        7   \n",
       "Goals in PSO              int64    128         0     0.000        4   \n",
       "Man of the Match         object    128         0     0.000        2   \n",
       "Off-Target                int64    128         0     0.000       11   \n",
       "Offsides                  int64    128         0     0.000        6   \n",
       "On-Target                 int64    128         0     0.000       12   \n",
       "Opponent                 object    128         0     0.000       32   \n",
       "Own goal Time           float64     12       116     0.906       11   \n",
       "Own goals               float64     12       116     0.906        1   \n",
       "PSO                      object    128         0     0.000        2   \n",
       "Pass Accuracy %           int64    128         0     0.000       25   \n",
       "Passes                    int64    128         0     0.000      110   \n",
       "Red                       int64    128         0     0.000        2   \n",
       "Round                    object    128         0     0.000        6   \n",
       "Saves                     int64    128         0     0.000       10   \n",
       "Team                     object    128         0     0.000       32   \n",
       "Yellow & Red              int64    128         0     0.000        2   \n",
       "Yellow Card               int64    128         0     0.000        7   \n",
       "\n",
       "                               min    25%    50%     75%           max  \\\n",
       "1st Goal                       1.0  18.25   39.0   54.75          90.0   \n",
       "Attempts                         3    9.0   12.0    15.0            26   \n",
       "Ball Possession %               25   42.0   50.0    58.0            75   \n",
       "Blocked                          0   1.75    3.0     4.0            10   \n",
       "Corners                          0    3.0    5.0     6.0            11   \n",
       "Date                    01-07-2018      -      -       -    30-06-2018   \n",
       "Distance Covered (Kms)          80  101.0  104.5   109.0           148   \n",
       "Fouls Committed                  5   10.0   13.0    16.0            25   \n",
       "Free Kicks                       5   11.0   15.0    18.0            26   \n",
       "Goal Scored                      0    0.0    1.0     2.0             6   \n",
       "Goals in PSO                     0    0.0    0.0     0.0             4   \n",
       "Man of the Match                No      -      -       -           Yes   \n",
       "Off-Target                       1    4.0    5.0     7.0            11   \n",
       "Offsides                         0    0.0    1.0     2.0             5   \n",
       "On-Target                        0    2.0    3.5     5.0            12   \n",
       "Opponent                 Argentina      -      -       -       Uruguay   \n",
       "Own goal Time                 12.0  21.75   35.0   75.75          90.0   \n",
       "Own goals                      1.0    1.0    1.0     1.0           1.0   \n",
       "PSO                             No      -      -       -           Yes   \n",
       "Pass Accuracy %                 67   79.0   83.0    87.0            94   \n",
       "Passes                         189  351.0  462.0  555.25          1137   \n",
       "Red                              0    0.0    0.0     0.0             1   \n",
       "Round                    3rd Place      -      -       -  Semi- Finals   \n",
       "Saves                            0    1.0    2.0     4.0             9   \n",
       "Team                     Argentina      -      -       -       Uruguay   \n",
       "Yellow & Red                     0    0.0    0.0     0.0             1   \n",
       "Yellow Card                      0    1.0    2.0     2.0             6   \n",
       "\n",
       "                              mean median         std      skew  \n",
       "1st Goal                 39.457447   39.0   24.496506  0.346921  \n",
       "Attempts                  12.59375   12.0    5.245827  0.653512  \n",
       "Ball Possession %        49.992188   50.0   10.444074  0.002231  \n",
       "Blocked                   3.359375    3.0    2.403195  0.953987  \n",
       "Corners                    4.71875    5.0    2.446072  0.306062  \n",
       "Date                             -      -           -         -  \n",
       "Distance Covered (Kms)  106.664062  104.5   11.749537  1.773204  \n",
       "Fouls Committed          13.546875   13.0    4.619131  0.268439  \n",
       "Free Kicks               14.890625   15.0    4.724262  0.100006  \n",
       "Goal Scored               1.320312    1.0    1.156519  1.145702  \n",
       "Goals in PSO              0.203125    0.0    0.807049  3.903529  \n",
       "Man of the Match                 -      -           -         -  \n",
       "Off-Target                5.273438    5.0    2.409675   0.28325  \n",
       "Offsides                   1.34375    1.0    1.193404  0.829584  \n",
       "On-Target                 3.914062    3.5    2.234403  0.802058  \n",
       "Opponent                         -      -           -         -  \n",
       "Own goal Time            45.833333   35.0   29.978275   0.50659  \n",
       "Own goals                      1.0    1.0         0.0       0.0  \n",
       "PSO                              -      -           -         -  \n",
       "Pass Accuracy %          82.554688   83.0    5.933766 -0.699259  \n",
       "Passes                  462.648438  462.0  151.186311  0.773985  \n",
       "Red                       0.015625    0.0    0.124507  7.904196  \n",
       "Round                            -      -           -         -  \n",
       "Saves                     2.726562    2.0    2.049447  0.945214  \n",
       "Team                             -      -           -         -  \n",
       "Yellow & Red              0.015625    0.0    0.124507  7.904196  \n",
       "Yellow Card               1.695312    2.0    1.325454  0.805754  "
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Explore\n",
    "explore(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill Missing\n",
    "df = clean(df, method='fillmissing')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Delete Outliers - must specify columns\n",
    "df = clean(df, method='outliers', columns=['Blocked','Corners'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>team</th>\n",
       "      <th>opponent</th>\n",
       "      <th>goal_scored</th>\n",
       "      <th>ball_possession_%</th>\n",
       "      <th>attempts</th>\n",
       "      <th>on-target</th>\n",
       "      <th>off-target</th>\n",
       "      <th>blocked</th>\n",
       "      <th>corners</th>\n",
       "      <th>...</th>\n",
       "      <th>yellow_card</th>\n",
       "      <th>yellow_&amp;_red</th>\n",
       "      <th>red</th>\n",
       "      <th>man_of_the_match</th>\n",
       "      <th>1st_goal</th>\n",
       "      <th>round</th>\n",
       "      <th>pso</th>\n",
       "      <th>goals_in_pso</th>\n",
       "      <th>own_goals</th>\n",
       "      <th>own_goal_time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>0 rows × 27 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [date, team, opponent, goal_scored, ball_possession_%, attempts, on-target, off-target, blocked, corners, offsides, free_kicks, saves, pass_accuracy_%, passes, distance_covered_(kms), fouls_committed, yellow_card, yellow_&_red, red, man_of_the_match, 1st_goal, round, pso, goals_in_pso, own_goals, own_goal_time]\n",
       "Index: []\n",
       "\n",
       "[0 rows x 27 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Clean Data -\n",
    "clean(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning data manually"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_data(df, test=False):\n",
    "    df = df.dropna(how='any', axis='rows')\n",
    "    df = df[(df.dropoff_latitude != 0) | (df.dropoff_longitude != 0)]\n",
    "    df = df[(df.pickup_latitude != 0) | (df.pickup_longitude != 0)]\n",
    "    df = df[df.fare_amount.between(0, 4000)]\n",
    "    df = df[df.passenger_count < 8]\n",
    "    df = df[df.passenger_count >= 0]\n",
    "    df = df[df[\"pickup_latitude\"].between(40, 42)]\n",
    "    df = df[df[\"pickup_longitude\"].between(-74.3, -72.9 )]\n",
    "    df = df[df[\"dropoff_latitude\"].between(40, 42)]\n",
    "    df = df[df[\"dropoff_longitude\"].between(-74, -72.9)]\n",
    "    return df\n",
    "\n",
    "df_cleaned = clean_data(df)\n",
    "\"% data removed\", (1 - len(df_cleaned) / len(df)) * 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing data by columns:\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-2d4e277b7a01>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Missing data by columns:\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misnull\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "print(\"Missing data by columns:\")\n",
    "\n",
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "['payment_amount']",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-36-642b565db913>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;31m# removing nans based on particular column\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropna\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msubset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'payment_amount'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minplace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.pyenv/versions/3.8.6/envs/lewagon/lib/python3.8/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36mdropna\u001b[0;34m(self, axis, how, thresh, subset, inplace)\u001b[0m\n\u001b[1;32m   5160\u001b[0m             \u001b[0mcheck\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mindices\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5161\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mcheck\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0many\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5162\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompress\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcheck\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msubset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5163\u001b[0m             \u001b[0magg_obj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0magg_axis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5164\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: ['payment_amount']"
     ]
    }
   ],
   "source": [
    "# remove all nan value rows. these can stop some statistical analysis running. e.g. seaborn pairplot. \n",
    "\n",
    "drop_nan_df = df.dropna()\n",
    "\n",
    "# df.dropna(inplace=True)\n",
    "# the above is a way of permanently modifying the dataframe\n",
    "\n",
    "# better practice than drop na would be to simply repopulate all nan value with suitable placeholders. \n",
    "\n",
    "# removing nans based on particular column\n",
    "df.dropna(subset=['payment_amount'], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Changing Dates format \n",
    "\n",
    "the commented line sets date as index which is often common practice. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fix_datetime_on_dataframe(df):\n",
    "    df['date'] = pd.to_datetime(df['date'])\n",
    "    #df.set_index(keys='date', inplace=True) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploratory Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summarising Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The quickda library introduced in the cleaning sections has some shortcuts for this. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Goal Scored</th>\n",
       "      <th>Ball Possession %</th>\n",
       "      <th>Attempts</th>\n",
       "      <th>On-Target</th>\n",
       "      <th>Off-Target</th>\n",
       "      <th>Blocked</th>\n",
       "      <th>Corners</th>\n",
       "      <th>Offsides</th>\n",
       "      <th>Free Kicks</th>\n",
       "      <th>Saves</th>\n",
       "      <th>...</th>\n",
       "      <th>Passes</th>\n",
       "      <th>Distance Covered (Kms)</th>\n",
       "      <th>Fouls Committed</th>\n",
       "      <th>Yellow Card</th>\n",
       "      <th>Yellow &amp; Red</th>\n",
       "      <th>Red</th>\n",
       "      <th>1st Goal</th>\n",
       "      <th>Goals in PSO</th>\n",
       "      <th>Own goals</th>\n",
       "      <th>Own goal Time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>118.000000</td>\n",
       "      <td>118.000000</td>\n",
       "      <td>118.000000</td>\n",
       "      <td>118.000000</td>\n",
       "      <td>118.000000</td>\n",
       "      <td>118.000000</td>\n",
       "      <td>118.000000</td>\n",
       "      <td>118.000000</td>\n",
       "      <td>118.000000</td>\n",
       "      <td>118.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>118.000000</td>\n",
       "      <td>118.000000</td>\n",
       "      <td>118.000000</td>\n",
       "      <td>118.000000</td>\n",
       "      <td>118.000000</td>\n",
       "      <td>118.000000</td>\n",
       "      <td>89.000000</td>\n",
       "      <td>118.000000</td>\n",
       "      <td>10.0</td>\n",
       "      <td>10.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>1.381356</td>\n",
       "      <td>48.915254</td>\n",
       "      <td>11.906780</td>\n",
       "      <td>3.796610</td>\n",
       "      <td>5.220339</td>\n",
       "      <td>2.889831</td>\n",
       "      <td>4.432203</td>\n",
       "      <td>1.381356</td>\n",
       "      <td>15.033898</td>\n",
       "      <td>2.754237</td>\n",
       "      <td>...</td>\n",
       "      <td>447.830508</td>\n",
       "      <td>106.762712</td>\n",
       "      <td>13.576271</td>\n",
       "      <td>1.694915</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>39.089888</td>\n",
       "      <td>0.194915</td>\n",
       "      <td>1.0</td>\n",
       "      <td>46.300000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>1.176112</td>\n",
       "      <td>9.762633</td>\n",
       "      <td>4.697676</td>\n",
       "      <td>2.170349</td>\n",
       "      <td>2.353886</td>\n",
       "      <td>1.834074</td>\n",
       "      <td>2.277646</td>\n",
       "      <td>1.218935</td>\n",
       "      <td>4.744646</td>\n",
       "      <td>2.037832</td>\n",
       "      <td>...</td>\n",
       "      <td>134.495595</td>\n",
       "      <td>11.686560</td>\n",
       "      <td>4.576803</td>\n",
       "      <td>1.310761</td>\n",
       "      <td>0.129631</td>\n",
       "      <td>0.092057</td>\n",
       "      <td>24.218532</td>\n",
       "      <td>0.798058</td>\n",
       "      <td>0.0</td>\n",
       "      <td>29.836778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>25.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>189.000000</td>\n",
       "      <td>80.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>12.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>42.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>11.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>348.750000</td>\n",
       "      <td>101.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>18.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>25.250000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>48.000000</td>\n",
       "      <td>12.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>15.000000</td>\n",
       "      <td>2.500000</td>\n",
       "      <td>...</td>\n",
       "      <td>451.000000</td>\n",
       "      <td>105.000000</td>\n",
       "      <td>13.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>39.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>35.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>2.000000</td>\n",
       "      <td>57.000000</td>\n",
       "      <td>14.750000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>18.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>546.250000</td>\n",
       "      <td>109.000000</td>\n",
       "      <td>16.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>53.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>72.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>6.000000</td>\n",
       "      <td>71.000000</td>\n",
       "      <td>25.000000</td>\n",
       "      <td>12.000000</td>\n",
       "      <td>11.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>26.000000</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>762.000000</td>\n",
       "      <td>148.000000</td>\n",
       "      <td>25.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>90.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>90.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Goal Scored  Ball Possession %    Attempts   On-Target  Off-Target  \\\n",
       "count   118.000000         118.000000  118.000000  118.000000  118.000000   \n",
       "mean      1.381356          48.915254   11.906780    3.796610    5.220339   \n",
       "std       1.176112           9.762633    4.697676    2.170349    2.353886   \n",
       "min       0.000000          25.000000    3.000000    0.000000    1.000000   \n",
       "25%       1.000000          42.000000    8.000000    2.000000    4.000000   \n",
       "50%       1.000000          48.000000   12.000000    3.000000    5.000000   \n",
       "75%       2.000000          57.000000   14.750000    5.000000    7.000000   \n",
       "max       6.000000          71.000000   25.000000   12.000000   11.000000   \n",
       "\n",
       "          Blocked     Corners    Offsides  Free Kicks       Saves  ...  \\\n",
       "count  118.000000  118.000000  118.000000  118.000000  118.000000  ...   \n",
       "mean     2.889831    4.432203    1.381356   15.033898    2.754237  ...   \n",
       "std      1.834074    2.277646    1.218935    4.744646    2.037832  ...   \n",
       "min      0.000000    0.000000    0.000000    5.000000    0.000000  ...   \n",
       "25%      1.000000    3.000000    0.250000   11.000000    1.000000  ...   \n",
       "50%      3.000000    5.000000    1.000000   15.000000    2.500000  ...   \n",
       "75%      4.000000    6.000000    2.000000   18.000000    4.000000  ...   \n",
       "max      7.000000   10.000000    5.000000   26.000000    9.000000  ...   \n",
       "\n",
       "           Passes  Distance Covered (Kms)  Fouls Committed  Yellow Card  \\\n",
       "count  118.000000              118.000000       118.000000   118.000000   \n",
       "mean   447.830508              106.762712        13.576271     1.694915   \n",
       "std    134.495595               11.686560         4.576803     1.310761   \n",
       "min    189.000000               80.000000         5.000000     0.000000   \n",
       "25%    348.750000              101.000000        10.000000     1.000000   \n",
       "50%    451.000000              105.000000        13.000000     2.000000   \n",
       "75%    546.250000              109.000000        16.000000     2.000000   \n",
       "max    762.000000              148.000000        25.000000     6.000000   \n",
       "\n",
       "       Yellow & Red         Red   1st Goal  Goals in PSO  Own goals  \\\n",
       "count    118.000000  118.000000  89.000000    118.000000       10.0   \n",
       "mean       0.016949    0.008475  39.089888      0.194915        1.0   \n",
       "std        0.129631    0.092057  24.218532      0.798058        0.0   \n",
       "min        0.000000    0.000000   1.000000      0.000000        1.0   \n",
       "25%        0.000000    0.000000  18.000000      0.000000        1.0   \n",
       "50%        0.000000    0.000000  39.000000      0.000000        1.0   \n",
       "75%        0.000000    0.000000  53.000000      0.000000        1.0   \n",
       "max        1.000000    1.000000  90.000000      4.000000        1.0   \n",
       "\n",
       "       Own goal Time  \n",
       "count      10.000000  \n",
       "mean       46.300000  \n",
       "std        29.836778  \n",
       "min        12.000000  \n",
       "25%        25.250000  \n",
       "50%        35.000000  \n",
       "75%        72.500000  \n",
       "max        90.000000  \n",
       "\n",
       "[8 rows x 21 columns]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe()\n",
    "\n",
    "#include categorical data also. \n",
    "#df.describe(include=['O'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def what(dataframe):\n",
    "    print(\"the number of donors recorded are {}, there are {} columns representing this data\".format(dataframe.shape[0],dataframe.shape[1]) )\n",
    "what(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"There are {} observations and {} features in this dataset. \\n\".format(df.shape[0],df.shape[1]))\n",
    "\n",
    "print(\"There are {} different responses in the dataset such as... {}\\n\".format(len(df.where_heard.unique()), df.where_heard[3]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Graphing \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.distplot(df.dob, color='g', bins=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#boxplot\n",
    "\n",
    "sns.boxplot(x = df_ft.sex, y = df_ft.payment_amount)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# seaplot pair plot compares all data sets with each other. \n",
    "\n",
    "sns.pairplot(data=drop_nan_df[['donation_amount','type']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#make histograms of all numerical data\n",
    "\n",
    "df_num.hist(figsize=(16,13),bins=10)\n",
    "\n",
    "#if needed i can drop all non numerical data and nan before running this. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(13,5))\n",
    "plt.suptitle('Regression of review_score, 95% confidence interval')\n",
    "plt.subplot(1,2,1)\n",
    "sns.regplot(x = sample.wait_time, y= sample.review_score, y_jitter=.1, ci=95)\n",
    "plt.xlim(right=70)\n",
    "plt.ylim(bottom=0)\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "sns.regplot(x = orders.delay_vs_expected, y= orders.review_score, y_jitter=.1, ci=95)\n",
    "plt.xlim(right=70)\n",
    "plt.ylim(bottom=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### grouped bars\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "df.sort_values(\"dob\").plot(y=[\"payment_amount\",\"payment_actual\"],\n",
    "                           x=\"dob\", kind=\"line\")\n",
    "\n",
    "sns.barplot(x=df.dob,y = df.payment_actual , color='g')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(orders.corr(), cmap='coolwarm')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Changing colour of points on basis of one column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.subplots(figsize=(20,8))\n",
    "sns.scatterplot(x='dob', y='payment_actual', data=df_dob, hue=\"sex\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### building subplots and scatter graphs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR PLOTS HERE\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "plt.subplots(1,2,figsize=(20,8))\n",
    "plt.subplot(121)\n",
    "sns.scatterplot(x='rating', y='admitted', data=df)\n",
    "plt.subplot(122)\n",
    "sns.scatterplot(x='CGPA', y='admitted', data=df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Debugging\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ipdb; ipdb.set_trace()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import ipdb will import the ipdb module while ipdb.set_trace() will allow us to halt the program at a certain line.\n",
    "\n",
    "Go back to the terminal and run the command again:\n",
    "\n",
    "python hello.py john lennon\n",
    "The program will halt at the line where you inserted the pdb.set_trace():\n",
    "\n",
    "> [...]data-challenges/01-Python/01-Programming-Basics/04-Debugging/hello.py(9)full_name()\n",
    "      8\n",
    "----> 9     name = f\"{first_name.capitalize()}{last_name.capitalize()}\"\n",
    "     10     return name\n",
    "\n",
    "ipdb>\n",
    "ℹ️ ipdb is not a module which is available by default in Python, so you need to pip install ipdb (something we did on Setup day). Alternatively, you can use the default pdb module embedded in Python:\n",
    "\n",
    "import pdb; pdb.set_trace()\n",
    "It’s time to play with the debugger. From there, you can do two things:\n",
    "\n",
    "Control the flow of the program, tell the debugger to execute the next line, step in a function or step out from it.\n",
    "Have a look at the current memory, basically what is stored in variables at this moment. The program is halted so that you can have a closer look to its internals.\n",
    "Type this:\n",
    "\n",
    "ipdb> sys.argv\n",
    " => ['hello.py', 'john', 'lennon']\n",
    "See how it works? You just asked the debugger to call sys.argv and look at what is stored in this list.\n",
    "\n",
    "Our problem is that there is a missing space between John and Lennon. So we would like to have a look at the local variable name. Let’s type:\n",
    "\n",
    "ipdb> name\n",
    " => *** NameError: name 'name' is not defined\n",
    "Why do we get this NameError? Where are we halted? To check at which line the program is halted, you can type:\n",
    "\n",
    "ipdb> ll\n",
    " 4     def full_name(first_name, last_name):\n",
    " 5         import pdb; pdb.set_trace()\n",
    " 6  ->     name = f\"{first_name.capitalize()}{last_name.capitalize()}\"\n",
    " 7         return name\n",
    "The program stopped before the line pointed at by the little arrow ->. This means that the name variable has not yet been assigned. This is why we get the “name is not defined” error. OK, everything is clear now!\n",
    "\n",
    "We are inside a function. Something useful is to display the argument list of the current function:\n",
    "\n",
    "ipdb> args\n",
    " first_name = 'john'\n",
    " last_name = 'lennon'\n",
    "What can we do now? We can ask the debugger to execute the next line with:\n",
    "\n",
    "ipdb> next\n",
    "Here you go, the debugger advanced by one line and executed it. You can see where the program is halted now with:\n",
    "\n",
    "ipdb> ll\n",
    "See how the little arrow -> advanced? Now we can check what’s inside the name variable:\n",
    "\n",
    "ipdb> name\n",
    " => 'JohnLennon'\n",
    "That’s it! We have identified the culprit! The interpolation is missing a space.\n",
    "\n",
    "You can let the program run until the next breakpoint (or the end of it) with:\n",
    "\n",
    "ipdb> continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calling python files in a notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To do this you can place the .py file that has the python code in it (e.g. copied from a notebook) in the notebook directory. \n",
    "\n",
    "You can then import this file using the following code. The first name (opengraph) is the name of the file without the extension. The second name is the name of a function within the python file. Once the function has been imported you can then run it as normal, giving it then ecessary arguments etc. \n",
    "\n",
    "#from opengraph import fetch_metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# API requests\n",
    "\n",
    "This should normally be done by first visiting the api page, by searching the site name and API. Then after that checking the basic documentation with how to access the data. \n",
    "\n",
    "The below code can be used to access the data. There are a variety of different ways this can be done but it is typically done using the request library and the get method. it can return one of three kinds of data types, these are specified in the api documentation but the most typical is json. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "def fetch_metadata(url):\n",
    "    \"\"\"\n",
    "    Return a dictionary of OpenGraph metadata found in HTML of given url\n",
    "    \"\"\"\n",
    "    response = requests.get(\"https://opengraph.lewagon.com\", params={'url': url})\n",
    "    if response.status_code == 200:\n",
    "        return response.json()[\"data\"]\n",
    "    return None\n",
    "\n",
    "urls_df = pd.read_csv(\"urls.csv\")\n",
    "urls_df\n",
    "\n",
    "#fetch_metadata(\"https://www.lewagon.com\")\n",
    "\n",
    "urls_df['title'] = ''\n",
    "urls_df['description'] = ''\n",
    "\n",
    "for index, row in urls_df.iterrows():\n",
    "    metadata = fetch_metadata(row['url'])\n",
    "    if metadata:\n",
    "        urls_df.loc[index, 'title'] = metadata['title']\n",
    "        urls_df.loc[index, 'description'] = metadata.get('description', '')\n",
    "        \n",
    "        \n",
    "        \n",
    "# another example of how to make an api request - simpler and faster. \n",
    "\n",
    "def create_stock_df_of_company(company_code):\n",
    "    \n",
    "    url = f'http://iex.lewagon.com/stable/stock/{company_code}/chart/3m'\n",
    "    df = pd.read_json(url)\n",
    "    return df\n",
    "\n",
    "\n",
    "#simplest of all\n",
    "pd.read_json('http://iex.lewagon.com/stable/stock/aapl/chart/3m')\n",
    "\n",
    "pd.read_json('https://api.covid19api.com/total/dayone/country/ireland')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scraping Data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "url = \"http://books.toscrape.com/\"\n",
    "\n",
    "# This is where we do an HTTP request to get the HTML from the website\n",
    "response = requests.get(url)\n",
    "\n",
    "# And this is where we feed that HTML to the Parser\n",
    "soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "# scrape the book classes\n",
    "books_html = soup.find_all(\"article\", class_=\"product_pod\")\n",
    "len(books_html)\n",
    "\n",
    "# Parsing _one_ book\n",
    "book_title = books_html[0].find(\"h3\").find(\"a\").attrs[\"title\"]\n",
    "book_title\n",
    "\n",
    "book_price = float(books_html[0].find(\"p\", class_=\"price_color\").string[1:])\n",
    "book_price\n",
    "\n",
    "book_stars_html = books_html[0].find(\"p\", class_=\"star-rating\")\n",
    "book_stars_html\n",
    "\n",
    "book_stars_html.attrs['class']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parsing _all_ books. same as above but shorter implementation and placement of data into a dictionary for storage. \n",
    "\n",
    "books_dict = { 'Title': [], 'Price': [], 'Rating': [] }\n",
    "\n",
    "for book in books_html:\n",
    "    title = book.find(\"h3\").find(\"a\").attrs[\"title\"]\n",
    "    price = float(book.find(\"p\", class_=\"price_color\").text[1:])\n",
    "    rating = book.find(\"p\", class_=\"star-rating\").attrs['class'][1]\n",
    "    books_dict[\"Title\"].append(title)\n",
    "    books_dict[\"Price\"].append(price)\n",
    "    books_dict[\"Rating\"].append(rating)\n",
    "    \n",
    "len(books_dict)         # You should have 3 dictionaries\n",
    "len(books_dict[\"Title\"]) # Each containing 20 elements from the 20 books, as many as on the web page!\n",
    "\n",
    "#loading data into pandas and plot\n",
    "books_df = pd.DataFrame.from_dict(books_dict)\n",
    "\n",
    "books_df.groupby(\"Rating\").count()[\"Title\"].plot(kind=\"bar\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### scraping multiple pages \n",
    "\n",
    "all_books_dict = { 'Title': [], 'Price': [], 'Rating': [] }\n",
    "\n",
    "MAX_PAGE = 30\n",
    "for page in range(1, MAX_PAGE + 1):\n",
    "    print(f\"Parsing page {page}...\")\n",
    "    url = f\"http://books.toscrape.com/catalogue/page-{page}.html\"\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "    for book in soup.find_all(\"article\", class_=\"product_pod\"):\n",
    "        title = book.find(\"h3\").find(\"a\").attrs[\"title\"]\n",
    "        price = float(book.find(\"p\", class_=\"price_color\").text[1:])\n",
    "        rating = book.find(\"p\", class_=\"star-rating\").attrs['class'][1]\n",
    "        all_books_dict[\"Title\"].append(title)\n",
    "        all_books_dict[\"Price\"].append(price)\n",
    "        all_books_dict[\"Rating\"].append(rating)\n",
    "all_books_df = pd.DataFrame.from_dict(all_books_dict)\n",
    "print(\"Done!\")\n",
    "all_books_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save all the scraped data into an excel file  or csv file so it is not lost when we close the notebook. \n",
    "all_books_df.to_csv(\"books.csv\")\n",
    "\n",
    "pip install xlsxwriter\n",
    "all_books_df.to_excel('books.xlsx', sheet_name='Books')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Easy frequency tables and simple text formatting\n",
    "\n",
    "gender_freq = {} \n",
    "for item in moma:\n",
    "    if item[5] not in gender_freq:\n",
    "        gender_freq[item[5]] = 1\n",
    "    else:\n",
    "        gender_freq[item[5]] += 1\n",
    "\n",
    "for gender,freq in gender_freq.items():\n",
    "    print(\"There are {:,} artworks by {} artists\".format(freq,gender))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning  - prepare the data set folder has a lot of stuff in their concisely. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-b067f449a6f7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#remove duplicates\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop_duplicates\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minplace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m#missing values\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "#remove duplicates \n",
    "\n",
    "df.drop_duplicates(inplace=True)\n",
    "\n",
    "#missing values\n",
    "\n",
    "df.isnull().sum().sort_values(ascending=False)/len(df) #NaN percentage for each column"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sampling datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = orders.sample(10000, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-Processing\n",
    "\n",
    "\n",
    "This is the best workbook i have - Preprocessing-Workflow_solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Imputation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# by changing 'median' to other commands e.g. 'most_frequent' we can change the kind of imputation that is done. \n",
    "\n",
    "import numpy as np\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "print(df.carwidth.unique()) # Check unique values in column\n",
    "\n",
    "df = df.replace(\"*\", np.nan) # Replace occurences of \"*\" by np.nan \n",
    "\n",
    "carwidth_imputer = SimpleImputer(strategy=\"median\") # Instanciate median imputer  \n",
    "\n",
    "carwidth_imputer.fit(df[['carwidth']]) # Fit imputer to carwidth column\n",
    "\n",
    "df['carwidth'] = carwidth_imputer.transform(df[['carwidth']]) # Impute\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scaling - find this in the ML car prices notebook ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "std_scaler = StandardScaler()\n",
    "\n",
    "df['curbweight'] = std_scaler.fit_transform(df[['curbweight']])\n",
    "\n",
    "df.head()\n",
    "\n",
    "\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "X_rescaled = scaler.fit_transform(X)\n",
    "\n",
    "cv_results = cross_validate(knn_model, X_rescaled,y)\n",
    "\n",
    "rescaled_score = cv_results['test_score'].mean()\n",
    "\n",
    "\n",
    "\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "\n",
    "rb_scaler = RobustScaler() \n",
    "\n",
    "df['peakrpm'],df['carwidth'],df['stroke'] = rb_scaler.fit_transform(df[['peakrpm','carwidth','stroke']]).T\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoding - also from car prices notebook ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#binary \n",
    "print(df.aspiration.unique()) # Check unique values of `aspiration`, these ones are binary. \n",
    "\n",
    "df.enginelocation.unique() # Check unique values of `enginelocation`\\\n",
    "\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "binary_encoder = OneHotEncoder(sparse=False, drop='if_binary')\n",
    "df['aspiration'], df['enginelocation'] = binary_encoder.fit_transform(df[['aspiration', 'enginelocation']]).T\n",
    "\n",
    "df.head()\n",
    "\n",
    "\n",
    "\n",
    "#ohe - makes a bunch of columns with a 1 or 0\n",
    "df.enginetype.unique() # Check unique values of `enginetype` , this one has many values\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "ohe = OneHotEncoder(sparse=False)\n",
    "\n",
    "enginetype_ohe = ohe.fit_transform(df[['enginetype']])\n",
    "\n",
    "print(ohe.categories_) # Check the column order returned by the transformation\n",
    "\n",
    "df['dohc'],df['dohcv'],df['l'],df['ohc'],df['ohcf'],df['ohcv'],df['rotor'] = enginetype_ohe.T\n",
    "\n",
    "df.drop(columns='enginetype', inplace = True)\n",
    "\n",
    "df.head()\n",
    "\n",
    "\n",
    "\n",
    "#manual  - gives a single column with a different number for each\n",
    "\n",
    "df['cylindernumber'] = df['cylindernumber'].map({'four': 4,\n",
    "                                                 'six': 6,\n",
    "                                                 \"five\":5,\n",
    "                                                 'three': 3,\n",
    "                                                 'twelve':12,\n",
    "                                                 'two':2,\n",
    "                                                 'eight':8})\n",
    "df.head()\n",
    "\n",
    "\n",
    "## label encoding the target. \n",
    "\n",
    "#     ℹ️ price is the target and must be Label encoded.\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "df['price'] = LabelEncoder().fit_transform(df['price'])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Selection\n",
    "\n",
    "see Feature-Selection_solution noptebook for me/ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.inspection import permutation_importance\n",
    "\n",
    "model = LogisticRegression().fit(X, y) # Fit model\n",
    "\n",
    "permutation_score = permutation_importance(model, X, y, n_repeats=100) # Perform Permutation\n",
    "\n",
    "importance_df = pd.DataFrame(np.vstack((X.columns,\n",
    "                                        permutation_score.importances_mean)).T) # Unstack results\n",
    "\n",
    "importance_df.columns=['feature','feature importance']\n",
    "\n",
    "importance_df.sort_values(by=\"feature importance\", ascending = False) # Order by importance\n",
    "\n",
    "\n",
    "# you can then drop the bad columns and retest with the improved model  as below\n",
    "\n",
    "X = df.drop(columns=['price','stroke','dohc','dohcv',\"l\",\"ohc\",\"enginelocation\"])\n",
    "\n",
    "model = LogisticRegression()\n",
    "\n",
    "scores = cross_val_score(model, X, y, cv=10)\n",
    "\n",
    "strong_model_score = scores.mean()\n",
    "\n",
    "strong_model_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checking correlations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-5510114afe1b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mseaborn\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msns\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mcorr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcorr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# Pearson Correlation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# Heatmap\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'data' is not defined"
     ]
    }
   ],
   "source": [
    "# make a chart - this is done better in the Feature-Selection_solution\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "corr = data.corr() # Pearson Correlation\n",
    "\n",
    "# Heatmap\n",
    "sns.heatmap(corr, \n",
    "        xticklabels=corr.columns,\n",
    "        yticklabels=corr.columns,\n",
    "        cmap= \"YlGnBu\")\n",
    "corr # this can also be sorted by target column if needs be. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "sort_values() missing 1 required positional argument: 'by'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-55-5625b65d207c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mdf_corr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcorr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mtop_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf_corr\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mabs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_corr\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0.12\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msort_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;31m#print(\"Top {} features strongly correlated with payment_amount: {}\".format(len(top_list),top_list))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: sort_values() missing 1 required positional argument: 'by'"
     ]
    }
   ],
   "source": [
    "#slightly different method\n",
    "\n",
    "# the df i call here is df_num, this is because for this we should only be coparing the numerical values. \n",
    "\n",
    "df_corr = df.corr()\n",
    "top_list = df_corr[abs(df_corr) > 0.12].sort_values('name of column you want to sort')\n",
    "#print(\"Top {} features strongly correlated with payment_amount: {}\".format(len(top_list),top_list))\n",
    "\n",
    "# its best to sort them in order of the highest correlations, of your target column, so that you can see which have the highest correlation.\n",
    "# i can also use the imputer though"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Permutation Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'eli5'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-56-e028bd98ae29>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0meli5\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0meli5\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msklearn\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPermutationImportance\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'eli5'"
     ]
    }
   ],
   "source": [
    "#Here is how to calculate and show importances with the eli5 library:\n",
    "\n",
    "\n",
    "\n",
    "import eli5\n",
    "from eli5.sklearn import PermutationImportance\n",
    "\n",
    "perm = PermutationImportance(my_model, random_state=1).fit(val_X, val_y)\n",
    "eli5.show_weights(perm, feature_names = val_X.columns.tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regression Model Example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'LinearRegression' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-2604166369a5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Instanciate the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLinearRegression\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Train the model on the Training data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'LinearRegression' is not defined"
     ]
    }
   ],
   "source": [
    "# Instanciate the model\n",
    "model = LinearRegression()\n",
    "\n",
    "# Train the model on the Training data\n",
    "model.fit(X, y)\n",
    "\n",
    "\n",
    "#Visualising the model \n",
    "# Define the line of best fit equation (using the slope and intercept values)\n",
    "# Plot it in a graph over the scattered data points\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Defining the line of best fit equation\n",
    "best_fit = slope * X + intercept\n",
    "\n",
    "# Plot!\n",
    "plt.scatter(X, y, alpha=0.9)\n",
    "plt.plot(X,best_fit, c=\"red\")\n",
    "plt.title('Scatter plot + Best Fit')\n",
    "plt.xlabel('Living area')\n",
    "plt.ylabel('Price')\n",
    "plt.show()\n",
    "\n",
    "# Make prediction with model\n",
    "prediction = model.predict([[1500]])[0]\n",
    "prediction\n",
    "\n",
    "\n",
    "# Scores\n",
    "cv_results['test_score']\n",
    "\n",
    "# Mean of scores\n",
    "cv_score = cv_results['test_score'].mean()\n",
    "\n",
    "cv_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest Classifier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Very basic Random Forets Classifier ML implementation. \n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# !kaggle datasets download -d mathan/fifa-2018-amatch-statistics\n",
    "\n",
    "data = pd.read_csv('datasets/ml/FIFA 2018 Statistics.csv')\n",
    "\n",
    "y = (data['Man of the Match'] == \"Yes\")  # Convert from string \"Yes\"/\"No\" to binary\n",
    "\n",
    "feature_names = [i for i in data.columns if data[i].dtype in [np.int64]]\n",
    "\n",
    "X = data[feature_names] #this will also delete the target data. \n",
    "\n",
    "train_X, val_X, train_y, val_y = train_test_split(X, y, random_state=1)\n",
    "\n",
    "my_model = RandomForestClassifier(n_estimators=100, random_state=0).fit(train_X, train_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "X_rescaled = scaler.fit_transform(X)\n",
    "\n",
    "cv_results = cross_validate(knn_model, X_rescaled,y)\n",
    "\n",
    "rescaled_score = cv_results['test_score'].mean()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('../data/titanic.csv')\n",
    "\n",
    "df.head()\n",
    "\n",
    "\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Create the feature set\n",
    "X = df[['Pclass','SibSp','Parch','Fare']]\n",
    "y = df['Survived']\n",
    "\n",
    "# Instanciate model\n",
    "model = LogisticRegression(max_iter=1000)\n",
    "\n",
    "# 5-Fold Cross validate model\n",
    "cv_results = cross_validate(model, X, y, cv=5)\n",
    "\n",
    "# Mean of accuracies\n",
    "accuracy = cv_results['test_score'].mean()\n",
    "\n",
    "accuracy\n",
    "\n",
    "\n",
    "# Predicting \n",
    "\n",
    "# Import the model\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Instanciate the model\n",
    "log_model = LogisticRegression()\n",
    "\n",
    "# Train the model on the training data\n",
    "log_model.fit(X_reduced, y_reduced)\n",
    "\n",
    "# Use the trained model to predict\n",
    "prediction = log_model.predict([[1,0,0,15]])[0]\n",
    "\n",
    "prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "check the ensemble methods challenge of le kitt. or check the pipelines challenge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Both Classification and Regression Problems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-Nearest-Neighbors\n",
    "\n",
    "one problem with this is that it gets slow with large data sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('https://wagon-public-datasets.s3.amazonaws.com/Machine%20Learning%20Datasets/ML_Houses_clean.csv')\n",
    "\n",
    "df.head()\n",
    "\n",
    "\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.model_selection import cross_validate\n",
    "\n",
    "X = df.drop(columns = ['SalePrice'])\n",
    "y = df.SalePrice\n",
    "\n",
    "# Instanciate the model\n",
    "knn_model = KNeighborsRegressor()\n",
    "\n",
    "# Train the model on the scaled Training data\n",
    "cv_results = cross_validate(knn_model, X,y)\n",
    "\n",
    "base_knn_score = cv_results['test_score'].mean()\n",
    "\n",
    "base_knn_score\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipelines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "check the ensemble methods challenge of le kitt. or check the pipelines challenge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Eli5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ELI5\n",
    "Machine learning models are not only about how accurate a model can predict but it’s also about how it predicts. Sometimes we need to understand which features are driving the predictions to optimize the model or explain it. For example, in a natural language processing classification problem, how can you easily see which words influenced the prediction? That’s precisely where Eli5 comes in.\n",
    "Eli5 helps you debugging machine learning classifiers and explain their predictions. It supports the most popular machine learning frameworks and packages, such as scikit-learn, Keras, XGBoost, LightGBM, and CatBoost. A while ago, I worked on an NLP project that classified hotel reviews, and I had to know which words were more influencing good and bad reviews the most. Eli5 was very handy. Let me show you how."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "unexpected EOF while parsing (<ipython-input-1-cb7d46398fdd>, line 5)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-1-cb7d46398fdd>\"\u001b[0;36m, line \u001b[0;32m5\u001b[0m\n\u001b[0;31m    eli5.explain_weights(model, feature_names = X_train.columns.values.tolist(), top = #_of_features)\u001b[0m\n\u001b[0m                                                                                                     ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m unexpected EOF while parsing\n"
     ]
    }
   ],
   "source": [
    "# Install Eli5\n",
    "!pip install eli5\n",
    "# Importing Eli5 \n",
    "from eli5.sklearn import explain_weights_sklearn\n",
    "eli5.explain_weights(model, feature_names = X_train.columns.values.tolist(), top = #_of_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "OperationalError",
     "evalue": "unable to open database file",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOperationalError\u001b[0m                          Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/tm/5y8q0cj507z0qzhr77sr05x80000gp/T/ipykernel_89349/1172826374.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msqlite3\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mconn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msqlite3\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconnect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'data/movies.sqlite'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0mdb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcursor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mdb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"YOUR SQL QUERY\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mOperationalError\u001b[0m: unable to open database file"
     ]
    }
   ],
   "source": [
    "#importing sql and a database.\n",
    "\n",
    "\n",
    "\n",
    "import sqlite3\n",
    "\n",
    "conn = sqlite3.connect('data/movies.sqlite')\n",
    "db = conn.cursor()\n",
    "db.execute(\"YOUR SQL QUERY\")\n",
    "rows = db.fetchall()\n",
    "print(rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def directors_count(db):\n",
    "    # return the number of directors contained in the database\n",
    "    # $CHALLENGIFY_BEGIN\n",
    "    query = \"\"\"\n",
    "        SELECT COUNT(*)\n",
    "        FROM directors\n",
    "    \"\"\"\n",
    "    db.execute(query)\n",
    "    count = db.fetchone()\n",
    "    return count[0]\n",
    "    # $CHALLENGIFY_END"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
